{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/spam-or-not-spam-dataset/spam_or_not_spam.csv\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "data=pd.read_csv(r'../input/spam-or-not-spam-dataset/spam_or_not_spam.csv')\n",
    "data.head()\n",
    "from sklearn.utils import shuffle\n",
    "data = shuffle(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3000"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['label'].value_counts()\n",
    "text =[] \n",
    "  \n",
    "# Iterate over each row \n",
    "for index, rows in data.iterrows(): \n",
    "    # Create list for the current row \n",
    "    my_list =str(rows.email)\n",
    "      \n",
    "    # append the list to the final list \n",
    "    text.append(my_list) \n",
    "  \n",
    "# Print the list \n",
    "len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "label=list(data['label'])\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "tokenizer = Tokenizer(num_words=1000)\n",
    "tokenizer.fit_on_texts(text)\n",
    "sequences = tokenizer.texts_to_sequences(text)\n",
    "x_train=sequences[:2000]\n",
    "y_train=label[:2000]\n",
    "x_test=sequences[2000:]\n",
    "y_test=label[2000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen = 20\n",
    "from keras import preprocessing\n",
    "x_train = preprocessing.sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = preprocessing.sequence.pad_sequences(x_test, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 20, 8)             16000     \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 32)                5248      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 21,281\n",
      "Trainable params: 21,281\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1600 samples, validate on 400 samples\n",
      "Epoch 1/10\n",
      "1600/1600 [==============================] - 1s 775us/step - loss: 0.4384 - acc: 0.8263 - val_loss: 0.2977 - val_acc: 0.8425\n",
      "Epoch 2/10\n",
      "1600/1600 [==============================] - 1s 336us/step - loss: 0.2193 - acc: 0.8981 - val_loss: 0.2111 - val_acc: 0.9150\n",
      "Epoch 3/10\n",
      "1600/1600 [==============================] - 1s 340us/step - loss: 0.1517 - acc: 0.9513 - val_loss: 0.1937 - val_acc: 0.9425\n",
      "Epoch 4/10\n",
      "1600/1600 [==============================] - 1s 336us/step - loss: 0.1115 - acc: 0.9700 - val_loss: 0.1780 - val_acc: 0.9400\n",
      "Epoch 5/10\n",
      "1600/1600 [==============================] - 1s 344us/step - loss: 0.0819 - acc: 0.9762 - val_loss: 0.1832 - val_acc: 0.9475\n",
      "Epoch 6/10\n",
      "1600/1600 [==============================] - 1s 337us/step - loss: 0.0610 - acc: 0.9787 - val_loss: 0.1397 - val_acc: 0.9450\n",
      "Epoch 7/10\n",
      "1600/1600 [==============================] - 1s 340us/step - loss: 0.0486 - acc: 0.9844 - val_loss: 0.1405 - val_acc: 0.9450\n",
      "Epoch 8/10\n",
      "1600/1600 [==============================] - 1s 336us/step - loss: 0.0379 - acc: 0.9875 - val_loss: 0.1739 - val_acc: 0.9500\n",
      "Epoch 9/10\n",
      "1600/1600 [==============================] - 1s 345us/step - loss: 0.0276 - acc: 0.9894 - val_loss: 0.2031 - val_acc: 0.9500\n",
      "Epoch 10/10\n",
      "1600/1600 [==============================] - 1s 340us/step - loss: 0.0252 - acc: 0.9919 - val_loss: 0.1849 - val_acc: 0.9525\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Flatten, Dense\n",
    "from keras.layers import Embedding,LSTM\n",
    "model = Sequential()\n",
    "model.add(Embedding(2000, 8, input_length=maxlen))\n",
    "model.add(LSTM(32))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
    "model.summary()\n",
    "history = model.fit(x_train, y_train,\n",
    "                    epochs=10,\n",
    "                    batch_size=32,\n",
    "                    validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 0s 93us/step\n",
      "test loss:0.126240263260901\n",
      "test accuracy:0.968999981880188\n"
     ]
    }
   ],
   "source": [
    "result=model.evaluate(x_test,y_test)\n",
    "print(\"test loss:{}\\ntest accuracy:{}\".format(result[0],result[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
